#Lexical Analyzer
The Lexical Analyzer project is an implementation of the fundamental concept in compiler design - the lexical analysis phase. In this phase, a compiler breaks down the statements of a program into lexical tokens, which are then used for further analysis to determine if the statements are syntactically correct.

**Introduction**
The Lexical Analyzer is an essential component of any compiler or interpreter. It serves as the initial step in the compilation process, where the source code is scanned and divided into tokens based on predefined rules. These tokens represent the smallest units of meaning in the programming language, such as keywords, identifiers, literals, and operators.

**Functionality**
The functionality of this lexical analyzer implementation includes:

Input Processing: Accepting a program or statements written in the target programming language as input.
Tokenization: Breaking down the input statements into lexical tokens according to the specified rules of the programming language.
How to Use
To use this lexical analyzer, follow these steps:

Run the Program: Execute the compiled program and provide the input program or statements as command-line arguments.

View Output: The program will process the input and generate lexical tokens as output.

**References**
To learn more about the concept of lexical analysis and compiler design, refer to the following resources:
Lexical Analysis - Wikipedia
Compiler Design: Lexical Analysis - GeeksforGeeks
